{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "328d13b7",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Vedant2100/w26/blob/main/CS%20226/vegetation_anomalies_project.ipynb)\n",
    "\n",
    "# Scalable Analysis of Vegetation Anomalies Preceding Plant Disease Outbreaks\n",
    "\n",
    "**Project:** Distributed Analysis using Apache Spark  \n",
    "**Team Size:** 5  \n",
    "**Platform:** Spark (Databricks or EMR)  \n",
    "**Storage:** S3 or HDFS  \n",
    "**Primary Data:** Sentinel-2 L2A, Landsat C2 L2  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "### Goal\n",
    "Analyze large-scale satellite imagery to identify vegetation anomalies that may precede plant disease outbreaks using distributed computing frameworks.\n",
    "\n",
    "### Key Deliverables\n",
    "- Clean, forest-masked, partitioned reflectance dataset\n",
    "- NDVI time series and historical baselines\n",
    "- Anomaly detection catalog with lead-lag correlation analysis\n",
    "- Scalability evaluation report\n",
    "- Interactive visualizations and final presentation\n",
    "\n",
    "### 4-Week Timeline\n",
    "- **Week 1:** Data Infrastructure & Preprocessing\n",
    "- **Week 2:** Vegetation Indices & Baseline Modeling\n",
    "- **Week 3:** Anomaly Detection & Validation\n",
    "- **Week 4:** Scaling, Visualization & Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0067a5",
   "metadata": {},
   "source": [
    "## Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aa8652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core PySpark Libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean, stddev, count, when, avg, sum as _sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Geospatial Processing\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    from sedona.spark import SedonaContext\n",
    "    from sedona.sql import st_constructors as ST\n",
    "    print(\"âœ“ Sedona/GeoSpark loaded\")\n",
    "except ImportError:\n",
    "    print(\"! Install Sedona: pip install apache-sedona\")\n",
    "\n",
    "# Raster Processing\n",
    "try:\n",
    "    import rasterio\n",
    "    from rasterio.features import rasterize\n",
    "    print(\"âœ“ Rasterio loaded\")\n",
    "except ImportError:\n",
    "    print(\"! Install rasterio: pip install rasterio\")\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Performance Tracking\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "print(\"\\n=== Environment Setup Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053adb9",
   "metadata": {},
   "source": [
    "## Storage Structure Configuration\n",
    "\n",
    "```\n",
    "s3://your-bucket/\n",
    "â”œâ”€â”€ raw/                  # Raw satellite imagery\n",
    "â”œâ”€â”€ processed/            # Quality-filtered data\n",
    "â”‚   â””â”€â”€ forest_masked/    # Forest-only pixels\n",
    "â”œâ”€â”€ ndvi/                 # NDVI time series\n",
    "â”œâ”€â”€ baseline/             # Historical baseline statistics\n",
    "â”œâ”€â”€ anomalies/            # Detected anomalies\n",
    "â””â”€â”€ reference/            # Outbreak reference data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BUCKET_NAME = \"your-bucket-name\"  # TODO: Update with your S3 bucket\n",
    "BASE_PATH = f\"s3://{BUCKET_NAME}/\"\n",
    "\n",
    "PATHS = {\n",
    "    'raw': f\"{BASE_PATH}raw/\",\n",
    "    'processed': f\"{BASE_PATH}processed/\",\n",
    "    'forest_masked': f\"{BASE_PATH}processed/forest_masked/\",\n",
    "    'ndvi': f\"{BASE_PATH}ndvi/\",\n",
    "    'baseline': f\"{BASE_PATH}baseline/\",\n",
    "    'anomalies': f\"{BASE_PATH}anomalies/\",\n",
    "    'reference': f\"{BASE_PATH}reference/\"\n",
    "}\n",
    "\n",
    "# Region of Interest (example bounding box)\n",
    "ROI = {\n",
    "    'min_lat': 35.0,\n",
    "    'max_lat': 45.0,\n",
    "    'min_lon': -125.0,\n",
    "    'max_lon': -110.0\n",
    "}\n",
    "\n",
    "# Time range\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2024\n",
    "\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad2ae70",
   "metadata": {},
   "source": [
    "---\n",
    "# WEEK 1: Data Infrastructure & Preprocessing\n",
    "ðŸŽ¯ **Goal:** Produce a clean, forest-masked, partitioned reflectance dataset ready for NDVI computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de7d6af",
   "metadata": {},
   "source": [
    "## Epic 5 â€” Story 5.1: Spark Cluster Configuration\n",
    "**Owner:** DevOps Lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ca547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session with optimized configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"VegetationAnomalyDetection\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Delta Lake support if available\n",
    "# .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "# .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print(f\"Workers: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fbe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Validation Test\n",
    "print(\"Testing cluster with simple read/write benchmark...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create test data\n",
    "test_data = spark.range(0, 10000000).selectExpr(\"id\", \"rand() as value\")\n",
    "\n",
    "# Write test\n",
    "test_path = f\"{BASE_PATH}test/benchmark\"\n",
    "test_data.write.mode(\"overwrite\").parquet(test_path)\n",
    "\n",
    "# Read test\n",
    "read_data = spark.read.parquet(test_path)\n",
    "result = read_data.agg({\"value\": \"avg\"}).collect()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"âœ“ Benchmark completed in {elapsed:.2f} seconds\")\n",
    "print(f\"âœ“ Partitions: {read_data.rdd.getNumPartitions()}\")\n",
    "\n",
    "# TODO: Document cluster configuration in separate file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc883b96",
   "metadata": {},
   "source": [
    "## Epic 1 â€” Story 1.1: Bulk Ingest Surface Reflectance\n",
    "**Owner:** Dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d731422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata schema for satellite imagery\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"tile_id\", StringType(), False),\n",
    "    StructField(\"satellite\", StringType(), False),\n",
    "    StructField(\"date\", TimestampType(), False),\n",
    "    StructField(\"year\", IntegerType(), False),\n",
    "    StructField(\"month\", IntegerType(), False),\n",
    "    StructField(\"region\", StringType(), False),\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"red_band\", StringType(), True),\n",
    "    StructField(\"nir_band\", StringType(), True),\n",
    "    StructField(\"qa_band\", StringType(), True)\n",
    "])\n",
    "\n",
    "def ingest_reflectance_data(years: List[int], roi: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Ingest surface reflectance data from Sentinel-2 and Landsat.\n",
    "    \n",
    "    TODO: Implement actual data ingestion logic:\n",
    "    - Query satellite data APIs (e.g., Google Earth Engine, AWS Open Data)\n",
    "    - Download tile references\n",
    "    - Convert to Spark DataFrames\n",
    "    - Partition by year, month, region\n",
    "    - Write to Parquet in /raw/\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Example: Create dummy metadata (replace with actual ingestion)\n",
    "    print(f\"Ingesting data for years {years[0]}-{years[-1]}...\")\n",
    "    print(f\"ROI: Lat({roi['min_lat']}, {roi['max_lat']}), Lon({roi['min_lon']}, {roi['max_lon']})\")\n",
    "    \n",
    "    # TODO: Implement data retrieval logic here\n",
    "    # Example structure:\n",
    "    # for year in years:\n",
    "    #     for month in range(1, 13):\n",
    "    #         tiles = query_satellite_api(year, month, roi)\n",
    "    #         df = create_spark_dataframe(tiles)\n",
    "    #         df.write.partitionBy(\"year\", \"month\", \"region\") \\\n",
    "    #           .parquet(f\"{PATHS['raw']}/reflectance\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nâœ“ Ingestion completed in {elapsed:.2f} seconds\")\n",
    "    # TODO: Log number of tiles ingested, total size, runtime\n",
    "\n",
    "# Run ingestion\n",
    "# ingest_reflectance_data(list(range(START_YEAR, END_YEAR + 1)), ROI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ace59",
   "metadata": {},
   "source": [
    "## Epic 1 â€” Story 1.2: Distributed Quality Filtering\n",
    "**Owner:** Dev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qa_filtering(input_path: str, output_path: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Apply distributed quality filtering to remove clouds and shadows.\n",
    "    \n",
    "    TODO: Implement QA band masking:\n",
    "    - Load raw reflectance dataset\n",
    "    - Identify QA band (varies by satellite)\n",
    "    - Decode bit flags for cloud/shadow\n",
    "    - Apply pixel-level mask\n",
    "    - Compute statistics on removed pixels\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"Loading data from {input_path}...\")\n",
    "    # df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # TODO: Implement QA masking logic\n",
    "    # Example for Landsat:\n",
    "    # - Bit 3: Cloud\n",
    "    # - Bit 4: Cloud Shadow\n",
    "    # \n",
    "    # def is_clear_pixel(qa_value):\n",
    "    #     return (qa_value & (1 << 3)) == 0 and (qa_value & (1 << 4)) == 0\n",
    "    # \n",
    "    # filtered_df = df.filter(is_clear_pixel(col(\"qa_band\")))\n",
    "    \n",
    "    # Compute statistics\n",
    "    # total_pixels = df.count()\n",
    "    # clean_pixels = filtered_df.count()\n",
    "    # percent_removed = ((total_pixels - clean_pixels) / total_pixels) * 100\n",
    "    \n",
    "    # Save cleaned data\n",
    "    # filtered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"âœ“ Quality filtering completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'runtime_seconds': elapsed,\n",
    "        # 'total_pixels': total_pixels,\n",
    "        # 'clean_pixels': clean_pixels,\n",
    "        # 'percent_removed': percent_removed\n",
    "    }\n",
    "\n",
    "# Run filtering\n",
    "# qa_stats = apply_qa_filtering(PATHS['raw'], PATHS['processed'])\n",
    "# print(json.dumps(qa_stats, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dac828",
   "metadata": {},
   "source": [
    "## Epic 1 â€” Story 1.3: Forest Masking via ESA WorldCover\n",
    "**Owner:** Dev3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ef2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_forest_mask(input_path: str, worldcover_path: str, output_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Apply forest mask using ESA WorldCover dataset.\n",
    "    \n",
    "    TODO: Implement forest masking:\n",
    "    - Ingest ESA WorldCover (10m resolution land cover)\n",
    "    - Extract forest class code (typically code 10)\n",
    "    - Perform spatial join with reflectance data\n",
    "    - Align resolutions (reproject if needed)\n",
    "    - Filter to forest-only pixels\n",
    "    - Save to /processed/forest_masked/\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Loading reflectance data...\")\n",
    "    # reflectance_df = spark.read.parquet(input_path)\n",
    "    \n",
    "    print(\"Loading ESA WorldCover land cover data...\")\n",
    "    # worldcover_df = spark.read.format(\"raster\").load(worldcover_path)\n",
    "    \n",
    "    # TODO: Extract forest pixels (class code 10 in ESA WorldCover)\n",
    "    # forest_mask = worldcover_df.filter(col(\"land_cover\") == 10)\n",
    "    \n",
    "    # TODO: Spatial join\n",
    "    # from sedona.sql.st_functions import ST_Intersects\n",
    "    # masked_df = reflectance_df.join(\n",
    "    #     forest_mask,\n",
    "    #     ST_Intersects(reflectance_df.geometry, forest_mask.geometry)\n",
    "    # )\n",
    "    \n",
    "    # Save masked data\n",
    "    # masked_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    # Validation\n",
    "    # total_pixels = reflectance_df.count()\n",
    "    # forest_pixels = masked_df.count()\n",
    "    # percent_forest = (forest_pixels / total_pixels) * 100\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"âœ“ Forest masking completed in {elapsed:.2f} seconds\")\n",
    "    # print(f\"âœ“ Forest coverage: {percent_forest:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'runtime_seconds': elapsed,\n",
    "        # 'total_pixels': total_pixels,\n",
    "        # 'forest_pixels': forest_pixels,\n",
    "        # 'percent_forest': percent_forest\n",
    "    }\n",
    "\n",
    "# Run forest masking\n",
    "# mask_stats = apply_forest_mask(\n",
    "#     PATHS['processed'],\n",
    "#     \"s3://esa-worldcover/v100/\",\n",
    "#     PATHS['forest_masked']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5434bf",
   "metadata": {},
   "source": [
    "---\n",
    "# WEEK 2: Vegetation Indices & Baseline Modeling\n",
    "ðŸŽ¯ **Goal:** Generate NDVI time series and compute historical baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882d7a4",
   "metadata": {},
   "source": [
    "## Epic 2 â€” Story 2.1: Scalable NDVI Computation\n",
    "**Owner:** Dev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ndvi(input_path: str, output_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute NDVI (Normalized Difference Vegetation Index) across all tiles.\n",
    "    \n",
    "    Formula: NDVI = (NIR - Red) / (NIR + Red)\n",
    "    \n",
    "    TODO: Implement distributed NDVI computation:\n",
    "    - Load forest-masked reflectance data\n",
    "    - Extract Red and NIR bands\n",
    "    - Apply NDVI formula element-wise\n",
    "    - Handle division by zero\n",
    "    - Partition by year, month, region\n",
    "    - Save to /ndvi/\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Loading forest-masked data from {input_path}...\")\n",
    "    # df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # TODO: Compute NDVI\n",
    "    # from pyspark.sql.functions import when\n",
    "    # \n",
    "    # ndvi_df = df.withColumn(\n",
    "    #     \"ndvi\",\n",
    "    #     when(\n",
    "    #         (col(\"nir\") + col(\"red\")) == 0,\n",
    "    #         None\n",
    "    #     ).otherwise(\n",
    "    #         (col(\"nir\") - col(\"red\")) / (col(\"nir\") + col(\"red\"))\n",
    "    #     )\n",
    "    # )\n",
    "    \n",
    "    # Filter valid NDVI range [-1, 1]\n",
    "    # ndvi_df = ndvi_df.filter((col(\"ndvi\") >= -1) & (col(\"ndvi\") <= 1))\n",
    "    \n",
    "    # Save with partitioning\n",
    "    # ndvi_df.write \\\n",
    "    #     .partitionBy(\"year\", \"month\", \"region\") \\\n",
    "    #     .mode(\"overwrite\") \\\n",
    "    #     .parquet(output_path)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"âœ“ NDVI computation completed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    # num_partitions = ndvi_df.rdd.getNumPartitions()\n",
    "    # total_records = ndvi_df.count()\n",
    "    \n",
    "    return {\n",
    "        'runtime_seconds': elapsed,\n",
    "        # 'num_partitions': num_partitions,\n",
    "        # 'total_records': total_records\n",
    "    }\n",
    "\n",
    "# Run NDVI computation\n",
    "# ndvi_stats = compute_ndvi(PATHS['forest_masked'], PATHS['ndvi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa6528",
   "metadata": {},
   "source": [
    "## Epic 2 â€” Story 2.2: Optional Multi-Index Exploration\n",
    "**Owner:** Dev4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f23369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_additional_indices(df):\n",
    "    \"\"\"\n",
    "    Compute additional vegetation indices for comparison.\n",
    "    \n",
    "    EVI (Enhanced Vegetation Index):\n",
    "    EVI = 2.5 * ((NIR - Red) / (NIR + 6*Red - 7.5*Blue + 1))\n",
    "    \n",
    "    NBR (Normalized Burn Ratio):\n",
    "    NBR = (NIR - SWIR) / (NIR + SWIR)\n",
    "    \n",
    "    TODO: Implement and compare multiple indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # EVI computation\n",
    "    # evi_df = df.withColumn(\n",
    "    #     \"evi\",\n",
    "    #     2.5 * ((col(\"nir\") - col(\"red\")) / \n",
    "    #            (col(\"nir\") + 6*col(\"red\") - 7.5*col(\"blue\") + 1))\n",
    "    # )\n",
    "    \n",
    "    # NBR computation (requires SWIR band)\n",
    "    # nbr_df = df.withColumn(\n",
    "    #     \"nbr\",\n",
    "    #     (col(\"nir\") - col(\"swir\")) / (col(\"nir\") + col(\"swir\"))\n",
    "    # )\n",
    "    \n",
    "    # TODO: Compute correlation between indices\n",
    "    # TODO: Visualize distributions\n",
    "    # TODO: Decide which to include in final analysis\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Optional: Run for exploratory analysis\n",
    "# compute_additional_indices(ndvi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd84ea",
   "metadata": {},
   "source": [
    "## Epic 3 â€” Story 3.1: Historical Baseline Construction\n",
    "**Owner:** Dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafcc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_baseline(input_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Compute historical baseline NDVI statistics by month.\n",
    "    \n",
    "    TODO: Implement baseline construction:\n",
    "    - Load NDVI time series\n",
    "    - Group by region, month (across all years)\n",
    "    - Compute mean and standard deviation\n",
    "    - Create seasonal baseline model\n",
    "    - Save to /baseline/\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Loading NDVI data from {input_path}...\")\n",
    "    # ndvi_df = spark.read.parquet(input_path)\n",
    "    \n",
    "    # TODO: Compute baseline statistics\n",
    "    # baseline_df = ndvi_df.groupBy(\"region\", \"month\") \\\n",
    "    #     .agg(\n",
    "    #         mean(\"ndvi\").alias(\"mean_ndvi\"),\n",
    "    #         stddev(\"ndvi\").alias(\"stddev_ndvi\"),\n",
    "    #         count(\"*\").alias(\"sample_count\")\n",
    "    #     )\n",
    "    \n",
    "    # Save baseline\n",
    "    # baseline_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"âœ“ Baseline computed in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    # TODO: Visualize seasonal curves\n",
    "\n",
    "# Run baseline computation\n",
    "# compute_baseline(PATHS['ndvi'], PATHS['baseline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0866d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Seasonal NDVI Baseline\n",
    "def plot_seasonal_baseline(baseline_path: str, region: str = None):\n",
    "    \"\"\"\n",
    "    Plot seasonal NDVI trends for selected region.\n",
    "    \"\"\"\n",
    "    # baseline_pdf = spark.read.parquet(baseline_path).toPandas()\n",
    "    \n",
    "    # if region:\n",
    "    #     baseline_pdf = baseline_pdf[baseline_pdf['region'] == region]\n",
    "    \n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(baseline_pdf['month'], baseline_pdf['mean_ndvi'], marker='o', label='Mean NDVI')\n",
    "    # plt.fill_between(\n",
    "    #     baseline_pdf['month'],\n",
    "    #     baseline_pdf['mean_ndvi'] - baseline_pdf['stddev_ndvi'],\n",
    "    #     baseline_pdf['mean_ndvi'] + baseline_pdf['stddev_ndvi'],\n",
    "    #     alpha=0.3, label='Â±1 Std Dev'\n",
    "    # )\n",
    "    # plt.xlabel('Month')\n",
    "    # plt.ylabel('NDVI')\n",
    "    # plt.title(f'Seasonal NDVI Baseline: {region or \"All Regions\"}')\n",
    "    # plt.legend()\n",
    "    # plt.grid(True, alpha=0.3)\n",
    "    # plt.show()\n",
    "    \n",
    "    pass\n",
    "\n",
    "# plot_seasonal_baseline(PATHS['baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962cc997",
   "metadata": {},
   "source": [
    "---\n",
    "# WEEK 3: Anomaly Detection & Validation\n",
    "ðŸŽ¯ **Goal:** Identify vegetation deviations and compare against outbreak timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0be65a",
   "metadata": {},
   "source": [
    "## Epic 3 â€” Story 3.2: Distributed Anomaly Identification\n",
    "**Owner:** Dev3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1abd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(ndvi_path: str, baseline_path: str, output_path: str, \n",
    "                     threshold: float = -2.0) -> None:\n",
    "    \"\"\"\n",
    "    Detect anomalous NDVI values using Z-score method.\n",
    "    \n",
    "    Z = (NDVI_current - Mean_baseline) / Std_baseline\n",
    "    Anomaly if Z < threshold (e.g., -2.0)\n",
    "    \n",
    "    TODO: Implement anomaly detection:\n",
    "    - Load NDVI and baseline data\n",
    "    - Join on region and month\n",
    "    - Compute Z-scores\n",
    "    - Flag anomalies\n",
    "    - Aggregate anomalous area per region/month\n",
    "    - Save to /anomalies/\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Loading NDVI and baseline data...\")\n",
    "    # ndvi_df = spark.read.parquet(ndvi_path)\n",
    "    # baseline_df = spark.read.parquet(baseline_path)\n",
    "    \n",
    "    # TODO: Join and compute Z-scores\n",
    "    # joined_df = ndvi_df.join(\n",
    "    #     baseline_df,\n",
    "    #     [\"region\", \"month\"],\n",
    "    #     \"left\"\n",
    "    # )\n",
    "    # \n",
    "    # anomaly_df = joined_df.withColumn(\n",
    "    #     \"z_score\",\n",
    "    #     (col(\"ndvi\") - col(\"mean_ndvi\")) / col(\"stddev_ndvi\")\n",
    "    # ).withColumn(\n",
    "    #     \"is_anomaly\",\n",
    "    #     when(col(\"z_score\") < threshold, 1).otherwise(0)\n",
    "    # )\n",
    "    \n",
    "    # TODO: Aggregate anomalies\n",
    "    # anomaly_summary = anomaly_df.groupBy(\"region\", \"year\", \"month\") \\\n",
    "    #     .agg(\n",
    "    #         _sum(\"is_anomaly\").alias(\"anomaly_count\"),\n",
    "    #         count(\"*\").alias(\"total_pixels\"),\n",
    "    #         (100.0 * _sum(\"is_anomaly\") / count(\"*\")).alias(\"percent_anomalous\")\n",
    "    #     )\n",
    "    \n",
    "    # Save anomalies\n",
    "    # anomaly_df.write.mode(\"overwrite\").parquet(f\"{output_path}/pixels\")\n",
    "    # anomaly_summary.write.mode(\"overwrite\").parquet(f\"{output_path}/summary\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"âœ“ Anomaly detection completed in {elapsed:.2f} seconds\")\n",
    "    print(f\"Threshold: Z < {threshold}\")\n",
    "\n",
    "# Run anomaly detection\n",
    "# detect_anomalies(PATHS['ndvi'], PATHS['baseline'], PATHS['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f18d95",
   "metadata": {},
   "source": [
    "## Epic 4 â€” Story 4.1: Outbreak Data Integration\n",
    "**Owner:** Dev4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d2d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_outbreak_data(outbreak_shapefile: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Ingest and standardize plant disease outbreak records.\n",
    "    \n",
    "    TODO: Implement outbreak data ingestion:\n",
    "    - Load outbreak shapefiles or CSV\n",
    "    - Standardize fields: date, region, disease_type, severity\n",
    "    - Convert to Spark spatial DataFrame\n",
    "    - Save to /reference/outbreaks/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example using GeoPandas + Spark\n",
    "    # gdf = gpd.read_file(outbreak_shapefile)\n",
    "    # \n",
    "    # # Standardize columns\n",
    "    # gdf = gdf.rename(columns={\n",
    "    #     'detection_date': 'date',\n",
    "    #     'location': 'region',\n",
    "    #     'disease': 'disease_type'\n",
    "    # })\n",
    "    # \n",
    "    # # Convert to Spark\n",
    "    # outbreak_df = spark.createDataFrame(gdf)\n",
    "    # \n",
    "    # # Save\n",
    "    # outbreak_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    \n",
    "    print(f\"âœ“ Outbreak data ingested to {output_path}\")\n",
    "\n",
    "# TODO: Provide path to outbreak data\n",
    "# ingest_outbreak_data(\"/path/to/outbreak.shp\", PATHS['reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1fb4e9",
   "metadata": {},
   "source": [
    "## Epic 4 â€” Story 4.2: Lead-Lag Correlation Analysis\n",
    "**Owner:** Dev5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lead_lag(anomaly_path: str, outbreak_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze temporal relationship between anomalies and outbreaks.\n",
    "    \n",
    "    TODO: Implement lead-lag analysis:\n",
    "    - Join anomalies with outbreak regions\n",
    "    - Compute time difference (anomaly spike -> outbreak detection)\n",
    "    - Calculate mean lead time and distribution\n",
    "    - Perform statistical test (t-test or Mann-Whitney)\n",
    "    - Generate summary tables\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading anomaly and outbreak data...\")\n",
    "    # anomaly_df = spark.read.parquet(f\"{anomaly_path}/summary\")\n",
    "    # outbreak_df = spark.read.parquet(outbreak_path)\n",
    "    \n",
    "    # TODO: Spatial-temporal join\n",
    "    # joined_df = anomaly_df.join(\n",
    "    #     outbreak_df,\n",
    "    #     (anomaly_df.region == outbreak_df.region) &\n",
    "    #     (anomaly_df.date <= outbreak_df.date),\n",
    "    #     \"inner\"\n",
    "    # )\n",
    "    \n",
    "    # TODO: Compute lead time\n",
    "    # from pyspark.sql.functions import datediff\n",
    "    # lead_lag_df = joined_df.withColumn(\n",
    "    #     \"lead_days\",\n",
    "    #     datediff(col(\"outbreak_date\"), col(\"anomaly_date\"))\n",
    "    # )\n",
    "    \n",
    "    # TODO: Statistical analysis\n",
    "    # results = lead_lag_df.groupBy(\"disease_type\") \\\n",
    "    #     .agg(\n",
    "    #         mean(\"lead_days\").alias(\"mean_lead_days\"),\n",
    "    #         stddev(\"lead_days\").alias(\"std_lead_days\"),\n",
    "    #         count(\"*\").alias(\"n_events\")\n",
    "    #     )\n",
    "    \n",
    "    # results_pdf = results.toPandas()\n",
    "    # return results_pdf\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Run lead-lag analysis\n",
    "# lead_lag_results = analyze_lead_lag(PATHS['anomalies'], PATHS['reference'])\n",
    "# print(lead_lag_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f581155",
   "metadata": {},
   "source": [
    "---\n",
    "# WEEK 4: Scaling, Visualization & Finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec7c15",
   "metadata": {},
   "source": [
    "## Epic 5 â€” Story 5.2: Scalability Evaluation\n",
    "**Owner:** DevOps + Dev1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab56255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_scalability():\n",
    "    \"\"\"\n",
    "    Evaluate horizontal scaling efficiency.\n",
    "    \n",
    "    TODO: Run NDVI computation on:\n",
    "    - 1 year of data\n",
    "    - 3 years of data\n",
    "    - Full dataset (2015-2024)\n",
    "    \n",
    "    Record: runtime, memory, CPU usage, data volume\n",
    "    Plot scaling curves\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for year_range in [1, 3, 10]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # TODO: Run NDVI pipeline on subset\n",
    "        # compute_ndvi(filtered_data, output)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results.append({\n",
    "            'years': year_range,\n",
    "            'runtime_seconds': elapsed,\n",
    "            # 'memory_gb': ...,\n",
    "            # 'data_volume_gb': ...\n",
    "        })\n",
    "    \n",
    "    # Plot results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Runtime vs Data Size\n",
    "    ax[0].plot(results_df['years'], results_df['runtime_seconds'], marker='o')\n",
    "    ax[0].set_xlabel('Years of Data')\n",
    "    ax[0].set_ylabel('Runtime (seconds)')\n",
    "    ax[0].set_title('Scaling: Runtime vs Data Volume')\n",
    "    ax[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Efficiency\n",
    "    # ax[1].plot(...)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('scalability_report.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run scalability benchmark\n",
    "# scalability_results = benchmark_scalability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168a9859",
   "metadata": {},
   "source": [
    "## Epic 6 â€” Story 6.1: Exploratory Visualization\n",
    "**Owner:** Dev5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6845e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomaly_heatmap(anomaly_path: str, outbreak_path: str, region: str = None):\n",
    "    \"\"\"\n",
    "    Create temporal heatmap of anomalies with outbreak overlays.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    # anomaly_summary = spark.read.parquet(f\"{anomaly_path}/summary\").toPandas()\n",
    "    # outbreaks = spark.read.parquet(outbreak_path).toPandas()\n",
    "    \n",
    "    # if region:\n",
    "    #     anomaly_summary = anomaly_summary[anomaly_summary['region'] == region]\n",
    "    #     outbreaks = outbreaks[outbreaks['region'] == region]\n",
    "    \n",
    "    # Create heatmap\n",
    "    # pivot_table = anomaly_summary.pivot_table(\n",
    "    #     values='percent_anomalous',\n",
    "    #     index='month',\n",
    "    #     columns='year',\n",
    "    #     aggfunc='mean'\n",
    "    # )\n",
    "    \n",
    "    # plt.figure(figsize=(14, 8))\n",
    "    # sns.heatmap(pivot_table, cmap='RdYlGn_r', annot=True, fmt='.1f', \n",
    "    #             cbar_kws={'label': '% Anomalous Area'})\n",
    "    # \n",
    "    # # Overlay outbreak markers\n",
    "    # for _, outbreak in outbreaks.iterrows():\n",
    "    #     plt.plot(outbreak['year'], outbreak['month'], 'X', \n",
    "    #              markersize=15, color='black', label='Outbreak')\n",
    "    # \n",
    "    # plt.title(f'NDVI Anomalies and Disease Outbreaks: {region or \"All Regions\"}')\n",
    "    # plt.xlabel('Year')\n",
    "    # plt.ylabel('Month')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    \n",
    "    pass\n",
    "\n",
    "# Create visualization\n",
    "# create_anomaly_heatmap(PATHS['anomalies'], PATHS['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series_dashboard(ndvi_path: str, anomaly_path: str, outbreak_path: str):\n",
    "    \"\"\"\n",
    "    Create interactive time series plots.\n",
    "    \n",
    "    TODO: Consider using Plotly or Bokeh for interactivity\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example static visualization\n",
    "    # ndvi_ts = spark.read.parquet(ndvi_path).toPandas()\n",
    "    # anomalies = spark.read.parquet(f\"{anomaly_path}/summary\").toPandas()\n",
    "    # outbreaks = spark.read.parquet(outbreak_path).toPandas()\n",
    "    \n",
    "    # Plot time series with anomalies highlighted\n",
    "    # fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    # \n",
    "    # ax.plot(ndvi_ts['date'], ndvi_ts['ndvi'], label='NDVI', alpha=0.7)\n",
    "    # ax.scatter(\n",
    "    #     anomalies['date'],\n",
    "    #     anomalies['ndvi'],\n",
    "    #     c='red',\n",
    "    #     label='Anomaly',\n",
    "    #     s=50,\n",
    "    #     alpha=0.6\n",
    "    # )\n",
    "    # \n",
    "    # for _, outbreak in outbreaks.iterrows():\n",
    "    #     ax.axvline(outbreak['date'], color='orange', linestyle='--', \n",
    "    #                alpha=0.5, label='Outbreak')\n",
    "    # \n",
    "    # ax.set_xlabel('Date')\n",
    "    # ax.set_ylabel('NDVI')\n",
    "    # ax.set_title('NDVI Time Series with Anomalies and Outbreaks')\n",
    "    # ax.legend()\n",
    "    # ax.grid(True, alpha=0.3)\n",
    "    # plt.show()\n",
    "    \n",
    "    pass\n",
    "\n",
    "# create_time_series_dashboard(PATHS['ndvi'], PATHS['anomalies'], PATHS['reference'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bb6c24",
   "metadata": {},
   "source": [
    "## Epic 6 â€” Story 6.2: Final Report & Presentation\n",
    "**All Members**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a9d3e",
   "metadata": {},
   "source": [
    "### Report Sections\n",
    "\n",
    "1. **Executive Summary**\n",
    "   - Project goals and motivation\n",
    "   - Key findings\n",
    "   - Scalability results\n",
    "\n",
    "2. **Data Pipeline Architecture**\n",
    "   - Data sources and volume\n",
    "   - Spark cluster configuration\n",
    "   - Storage structure and partitioning strategy\n",
    "   - Processing stages (ingestion â†’ masking â†’ NDVI â†’ anomalies)\n",
    "\n",
    "3. **Methodology**\n",
    "   - Quality filtering approach\n",
    "   - Forest masking using ESA WorldCover\n",
    "   - NDVI computation and baseline modeling\n",
    "   - Anomaly detection algorithm (Z-score threshold)\n",
    "\n",
    "4. **Results**\n",
    "   - NDVI time series patterns\n",
    "   - Anomaly frequency and spatial distribution\n",
    "   - Lead-lag correlation with outbreaks\n",
    "   - Statistical significance tests\n",
    "\n",
    "5. **Scalability Analysis**\n",
    "   - Runtime vs data volume\n",
    "   - Horizontal scaling efficiency\n",
    "   - Memory and CPU utilization\n",
    "   - Bottleneck identification\n",
    "\n",
    "6. **Limitations**\n",
    "   - **No predictive claims**: This is exploratory correlation analysis only\n",
    "   - Cloud contamination despite QA filtering\n",
    "   - Temporal resolution constraints\n",
    "   - Outbreak data completeness\n",
    "\n",
    "7. **Future Work**\n",
    "   - Scale to petabyte-level global analysis\n",
    "   - Incorporate additional data sources (weather, soil)\n",
    "   - Machine learning for predictive modeling\n",
    "   - Real-time anomaly detection pipeline\n",
    "\n",
    "8. **Conclusions**\n",
    "   - Summary of findings\n",
    "   - Implications for early warning systems\n",
    "   - Distributed computing lessons learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9020f",
   "metadata": {},
   "source": [
    "---\n",
    "## Performance Metrics Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for tracking all metrics across pipeline stages\n",
    "performance_metrics = {\n",
    "    'ingestion': {\n",
    "        'runtime_seconds': None,\n",
    "        'tiles_ingested': None,\n",
    "        'data_volume_gb': None\n",
    "    },\n",
    "    'qa_filtering': {\n",
    "        'runtime_seconds': None,\n",
    "        'pixels_removed_pct': None\n",
    "    },\n",
    "    'forest_masking': {\n",
    "        'runtime_seconds': None,\n",
    "        'forest_coverage_pct': None\n",
    "    },\n",
    "    'ndvi_computation': {\n",
    "        'runtime_seconds': None,\n",
    "        'num_partitions': None,\n",
    "        'total_records': None\n",
    "    },\n",
    "    'anomaly_detection': {\n",
    "        'runtime_seconds': None,\n",
    "        'anomalies_detected': None\n",
    "    },\n",
    "    'scalability': {\n",
    "        '1_year_runtime': None,\n",
    "        '3_year_runtime': None,\n",
    "        'full_dataset_runtime': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "# with open('performance_metrics.json', 'w') as f:\n",
    "#     json.dump(performance_metrics, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dfb8bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Risk Mitigation Checklist\n",
    "\n",
    "### If data volume is too large:\n",
    "- âœ… Reduce ROI but maintain distributed framework\n",
    "- âœ… Focus on 2-3 years instead of full decade\n",
    "- âœ… Sample tiles instead of processing all\n",
    "\n",
    "### If anomaly signal is weak:\n",
    "- âœ… Emphasize exploratory findings\n",
    "- âœ… Document methodology thoroughly\n",
    "- âœ… Discuss data quality and limitations\n",
    "\n",
    "### If cluster is unstable:\n",
    "- âœ… Reduce number of workers\n",
    "- âœ… Optimize partition sizes\n",
    "- âœ… Increase executor memory\n",
    "- âœ… Enable checkpointing for long jobs\n",
    "\n",
    "### If processing is too slow:\n",
    "- âœ… Profile code to identify bottlenecks\n",
    "- âœ… Optimize shuffle operations\n",
    "- âœ… Use broadcast joins for small datasets\n",
    "- âœ… Cache intermediate results strategically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf636c",
   "metadata": {},
   "source": [
    "---\n",
    "## Team Collaboration Notes\n",
    "\n",
    "**Weekly Standups:**\n",
    "- Monday: Week planning and task assignment\n",
    "- Friday: Progress review and blocker discussion\n",
    "\n",
    "**Code Review:**\n",
    "- All major pipeline changes require 1 reviewer approval\n",
    "- Focus on efficiency, correctness, documentation\n",
    "\n",
    "**Documentation:**\n",
    "- Keep this notebook updated with latest code\n",
    "- Document all parameter choices and thresholds\n",
    "- Record runtime metrics for each stage\n",
    "\n",
    "**Communication Channels:**\n",
    "- Slack for quick questions\n",
    "- GitHub Issues for bugs and feature requests\n",
    "- Weekly meetings for strategic decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df49604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup: Stop Spark session when done\n",
    "# spark.stop()\n",
    "# print(\"Spark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
