{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Data Augmentation Experiments with ResNet18\n",
    "This assignment explores the benefits of different data augmentation techniques (Mixup, Cutout, Standard) \n",
    "on the learning performance of a ResNet18 model trained on a subset of CIFAR10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.models \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "def format_data(split):\n",
    "    images = np.array(split[\"img\"]) \n",
    "    labels = np.array(split[\"label\"])\n",
    "    images = images.transpose(0, 3, 1, 2).astype(np.float32) / 255.0\n",
    "    mean = images.mean(axis=(2, 3), keepdims=True)\n",
    "    std = images.std(axis=(2, 3), keepdims=True)\n",
    "    std[std == 0] = 1.0 \n",
    "    images = (images - mean) / std\n",
    "    return images, labels\n",
    "\n",
    "X_train_full, y_train_full = format_data(dataset[\"train\"])\n",
    "X_test, y_test = format_data(dataset[\"test\"])\n",
    "\n",
    "train_idxs = []\n",
    "for i in range(10):\n",
    "    class_idxs = np.where(y_train_full == i)[0]\n",
    "    sampled_idxs = np.random.choice(class_idxs, 1000, replace=False)\n",
    "    train_idxs.append(sampled_idxs)\n",
    "train_idxs = np.concatenate(train_idxs)\n",
    "np.random.shuffle(train_idxs)\n",
    "\n",
    "X_train = X_train_full[train_idxs]\n",
    "y_train = y_train_full[train_idxs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model and Training Loop Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNetModel():\n",
    "    model = torchvision.models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, augmentation_fn=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        if augmentation_fn:\n",
    "            inputs.requires_grad = True \n",
    "            inputs, targets = augmentation_fn(inputs, targets)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if targets.dim() > 1:\n",
    "            loss = criterion(outputs, targets)\n",
    "        else:\n",
    "            loss = criterion(outputs, targets)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Accuracy is only well-defined for hard labels in this simple loop\n",
    "    epoch_acc = 100. * correct / total if total > 0 else 0\n",
    "    return running_loss / len(loader), epoch_acc\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    return 100. * correct / total\n",
    "\n",
    "def train_model_aug(augmentation_fn=None, epochs=10, lr=0.001):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    model = ResNetModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    def criterion(outputs, targets):\n",
    "        if targets.dim() > 1:\n",
    "            return torch.mean(torch.sum(-targets * F.log_softmax(outputs, dim=1), dim=1))\n",
    "        return F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train).long())\n",
    "    test_ds = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test).long())\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=100, shuffle=False)\n",
    "    \n",
    "    report_loader = DataLoader(train_ds, batch_size=100, shuffle=False)\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'test_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss, _ = train_one_epoch(model, train_loader, optimizer, criterion, device, augmentation_fn)\n",
    "        train_acc = evaluate(model, report_loader, device)\n",
    "        test_acc = evaluate(model, test_loader, device)\n",
    "        \n",
    "        history['train_loss'].append(loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "    return history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation Implementations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_aug(inputs, targets, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    index = torch.randperm(batch_size).to(inputs.device)\n",
    "\n",
    "    mixed_x = lam * inputs + (1 - lam) * inputs[index, :]\n",
    "    \n",
    "    y_a = F.one_hot(targets, 10).float()\n",
    "    y_b = F.one_hot(targets[index], 10).float()\n",
    "    mixed_y = lam * y_a + (1 - lam) * y_b\n",
    "    \n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "def cutout_aug(inputs, K=16):\n",
    "    outputs = inputs.clone()\n",
    "    batch_size, channels, h, w = outputs.shape\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        if np.random.rand() > 0.5:\n",
    "            center_h = np.random.randint(0, h)\n",
    "            center_w = np.random.randint(0, w)\n",
    "            \n",
    "            y1 = np.clip(center_h - K // 2, 0, h)\n",
    "            y2 = np.clip(center_h + K // 2, 0, h)\n",
    "            x1 = np.clip(center_w - K // 2, 0, w)\n",
    "            x2 = np.clip(center_w + K // 2, 0, w)\n",
    "            \n",
    "            outputs[i, :, y1:y2, x1:x2] = 0\n",
    "            \n",
    "    return outputs\n",
    "\n",
    "def standard_aug(inputs, K=4):\n",
    "    batch_size, channels, h, w = inputs.shape\n",
    "    outputs = inputs.clone()\n",
    "    \n",
    "    flip = torch.rand(batch_size) > 0.5\n",
    "    outputs[flip] = torch.flip(outputs[flip], dims=[3])\n",
    "    \n",
    "    padded = F.pad(outputs, (K, K, K, K), mode='constant', value=0)\n",
    "    final_outputs = torch.zeros_like(outputs)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        k1 = np.random.randint(0, 2*K + 1)\n",
    "        k2 = np.random.randint(0, 2*K + 1)\n",
    "        final_outputs[i] = padded[i, :, k1:k1+h, k2:k2+w]\n",
    "        \n",
    "    return final_outputs\n",
    "\n",
    "def get_mixup_fn(alpha):\n",
    "    return lambda x, y: mixup_aug(x, y, alpha)\n",
    "\n",
    "def cutout_fn(x, y):\n",
    "    return cutout_aug(x, K=16), y\n",
    "\n",
    "def standard_fn(x, y):\n",
    "    return standard_aug(x, K=4), y\n",
    "\n",
    "def combined_fn(alpha):\n",
    "    def augment(x, y):\n",
    "        x_aug, y_aug = standard_fn(x, y)\n",
    "        x_aug, y_aug = cutout_fn(x_aug, y_aug)\n",
    "        return mixup_aug(x_aug, y_aug, alpha)\n",
    "    return augment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: No Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Task 1: No Augmentation\")\n",
    "history_none = train_model_aug(augmentation_fn=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Mixup Augmentation ($\\alpha=0.2, 0.4$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTask 2: Mixup (alpha=0.2)\")\n",
    "history_mixup_02 = train_model_aug(augmentation_fn=get_mixup_fn(0.2))\n",
    "\n",
    "print(\"\\nTask 2: Mixup (alpha=0.4)\")\n",
    "history_mixup_04 = train_model_aug(augmentation_fn=get_mixup_fn(0.4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Cutout Augmentation ($K=16$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTask 3: Cutout (K=16)\")\n",
    "history_cutout = train_model_aug(augmentation_fn=cutout_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Standard Augmentation ($K=4$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTask 4: Standard (K=4)\")\n",
    "history_standard = train_model_aug(augmentation_fn=standard_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Combined Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = 0.2 if history_mixup_02['test_acc'][-1] > history_mixup_04['test_acc'][-1] else 0.4\n",
    "print(f\"\\nTask 5: Combined (Standard + Cutout + Mixup alpha={best_alpha})\")\n",
    "history_combined = train_model_aug(augmentation_fn=combined_fn(best_alpha))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and Plots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(histories, titles):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Training Loss\n",
    "    for i, hist in enumerate(histories):\n",
    "        axes[0].plot(hist['train_loss'], label=titles[i])\n",
    "    axes[0].set_title('Training Loss over Epochs')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Training Accuracy\n",
    "    for i, hist in enumerate(histories):\n",
    "        axes[1].plot(hist['train_acc'], label=titles[i])\n",
    "    axes[1].set_title('Training Accuracy over Epochs')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Test Accuracy\n",
    "    for i, hist in enumerate(histories):\n",
    "        axes[2].plot(hist['test_acc'], label=titles[i])\n",
    "    axes[2].set_title('Test Accuracy over Epochs')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Accuracy (%)')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "histories = [\n",
    "    history_none, \n",
    "    history_mixup_02, \n",
    "    history_mixup_04, \n",
    "    history_cutout, \n",
    "    history_standard, \n",
    "    history_combined\n",
    "]\n",
    "titles = [\n",
    "    'None', \n",
    "    'Mixup 0.2', \n",
    "    'Mixup 0.4', \n",
    "    'Cutout', \n",
    "    'Standard', \n",
    "    'Combined'\n",
    "]\n",
    "\n",
    "plot_results(histories, titles)\n",
    "\n",
    "# Recording final test accuracy\n",
    "for title, hist in zip(titles, histories):\n",
    "    print(f\"Final Test Accuracy for {title}: {hist['test_acc'][-1]:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
