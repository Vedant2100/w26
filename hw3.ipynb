{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vedant2100/w26/blob/main/hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhtJKKrygzd"
      },
      "source": [
        "# Q1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JSyZ60AJ-JO"
      },
      "source": [
        " Normalization using train data mean and train data standard dev to both training and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "q1_c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "dataset = load_dataset(\"ylecun/mnist\")\n",
        "\n",
        "def format_data(split, mean=None, std=None):\n",
        "    images = np.array([np.array(img).flatten() for img in split[\"image\"]])\n",
        "    if mean is None or std is None:\n",
        "        mean = np.mean(images, axis=0)\n",
        "        std = np.std(images, axis=0)\n",
        "    std_copy = std.copy()\n",
        "    std_copy[std_copy == 0] = 1.0\n",
        "    X = (images - mean) / std_copy\n",
        "    y = np.array(split[\"label\"])\n",
        "    return X, y, mean, std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0MpXGptPqWZ"
      },
      "source": [
        "# Q2. MNIST models with diff k and p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byZzDgCyQLKj"
      },
      "source": [
        "##2.1\n",
        " I think there might be a typo in the question, because no dropout regularisation would correpsond to p = 0, not p = 1 as mention in the question. With p = 0.0, as k (the number of units )increases, the training accuracy and test accuracy both improve. At no k in the current set, the training accuracy has become 100%. I tried out a higher learning rate than the default value of 0.001, 0.01 and the training accuracy did get to 100% on k = 35."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2_c",
        "outputId": "fd078d8e-e318-4604-d1b4-c017d4505ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_train: (60000,)\n",
            "Shape of y_test: (10000,)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, train_mean, train_std = format_data(dataset[\"train\"])\n",
        "X_test, y_test, _, _ = format_data(dataset[\"test\"], train_mean, train_std)\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KfiqZzCKnDNE"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "idxs = []\n",
        "for c in range(10):\n",
        "    class_idxs = np.where(y_train == c)[0]\n",
        "    chosen = np.random.choice(class_idxs, size=1000, replace=True)\n",
        "    idxs.append(chosen)\n",
        "idxs = np.concatenate(idxs)\n",
        "np.random.shuffle(idxs)\n",
        "X_train = X_train[idxs]\n",
        "y_train = y_train[idxs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "hTe1raQZnDNF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class shallowNN(nn.Module):\n",
        "    def __init__(self, k = 1, p = 0.1):\n",
        "        super(shallowNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, k)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.fc2 = nn.Linear(k, 10)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = shallowNN()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "def train_model_k_p(X_train, y_train, X_test, y_test, p, k, epochs = 80, lr = 0.001):\n",
        "    model = shallowNN(k=k, p=p)\n",
        "    model.apply(init_weights)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "    train_acc = {}\n",
        "    test_acc = {}\n",
        "    loss_hist = {}\n",
        "    for epoch in tqdm.tqdm(range(epochs)):\n",
        "        model.train()\n",
        "        inputs = torch.tensor(X_train, dtype=torch.float32)\n",
        "        labels = torch.tensor(y_train, dtype=torch.long)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss_hist[(k, p, epoch)] = loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            train_acc[(k, p, epoch)] = (model(inputs).argmax(dim=1) == labels).float().mean().item()\n",
        "            test_outputs = model(torch.tensor(X_test, dtype=torch.float32))\n",
        "            test_acc[(k, p, epoch)] = (test_outputs.argmax(dim=1) == torch.tensor(y_test, dtype=torch.long)).float().mean().item()\n",
        "            model.train()\n",
        "    return train_acc, test_acc, loss_hist\n"
      ],
      "metadata": {
        "id": "oj3f5RIoOWJY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LR = 0.001 (default)"
      ],
      "metadata": {
        "id": "vlhXaHJ0qzTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "kvals = [1, 5, 15, 35]\n",
        "pvals = [1.0]\n",
        "epochs = 80\n",
        "for p in pvals:\n",
        "    for k in kvals:\n",
        "         train_acc, test_acc, loss_hist =  train_model_k_p(X_train, y_train, X_test, y_test, p, k, epochs, lr = 0.001)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xticks(kvals)\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Train Accuracy')\n",
        "    plt.plot(kvals, [test_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Test Accuracy')\n",
        "    plt.xlabel('Number of Neurons (k)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Accuracy vs Number of Neurons (p={p})')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    print(f\"\\nFinal train accuracy k = {k}, p = {p}: {train_acc[(k, p, epochs-1)]}\")\n",
        "    print(f\"\\nFinal test accuracy k = {k}, p = {p}: {test_acc[(k, p, epochs-1)]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "JArf-kZcqBYj",
        "outputId": "8f001a99-61e6-4b0c-8774-4b234c99873b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:02<00:00, 27.22it/s]\n",
            "100%|██████████| 80/80 [00:03<00:00, 21.55it/s]\n",
            "100%|██████████| 80/80 [00:03<00:00, 20.56it/s]\n",
            "100%|██████████| 80/80 [00:05<00:00, 15.51it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "(1, 1.0, 79)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-677046046.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkvals\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of Neurons (k)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (1, 1.0, 79)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0MAAAH/CAYAAACVVwbIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHZdJREFUeJzt3X1s1eX98PFPASkSbX1AimAVn50ywIHW+rDfjNWKho25JYhuMMZcNMaojRHrFESndXMSloASUafJRmAj6pbpMNqNmcUuRJCoiQ8DdTBnK2hotc52a3v/sdz17g0op7T0J5/XKzl/nIvr+n6vQzgt73zPQ1FXV1dXAAAAJDNooDcAAAAwEMQQAACQkhgCAABSEkMAAEBKYggAAEhJDAEAACmJIQAAICUxBAAApCSGAACAlMQQAACQUsEx9Nxzz8XUqVNj9OjRUVRUFE888cTnrlmzZk185StfieLi4jjuuOPikUce6cVWAQAA+k7BMdTa2hoTJkyIJUuW7Nb8t956Ky6++OI499xzY8OGDXHdddfFD37wg3j66acL3iwAAEBfKerq6urq9eKionj88cdj2rRpu5wzd+7cePLJJ+OVV17pHrv00ktj+/btsXr16t6eGgAAYI8M6e8TNDQ0RFVVVY+x6urquO6663a5pq2tLdra2rrvd3Z2xgcffBCHHnpoFBUV9ddWAQCA/+W6urriww8/jNGjR8egQXv2EQj9HkONjY1RVlbWY6ysrCxaWlriX//6V+y///47rKmrq4sFCxb099YAAIAvqC1btsQRRxyxR8fo9xjqjdra2qipqem+39zcHEceeWRs2bIlSkpKBnBnAADAQGppaYny8vI48MAD9/hY/R5Do0aNiqamph5jTU1NUVJSstOrQhERxcXFUVxcvMN4SUmJGAIAAPrk7TP9/j1DlZWVUV9f32PsmWeeicrKyv4+NQAAwC4VHEMfffRRbNiwITZs2BAR//3o7A0bNsTmzZsj4r8vcZs5c2b3/CuvvDLefPPNuPHGG+O1116L++67L37961/H9ddf3zePAAAAoBcKjqEXXnghTj311Dj11FMjIqKmpiZOPfXUmDdvXkREvPvuu91hFBFx9NFHx5NPPhnPPPNMTJgwIe6999548MEHo7q6uo8eAgAAQOH26HuG9paWlpYoLS2N5uZm7xkCAIDE+rIN+v09QwAAAP8biSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAAp9SqGlixZEmPHjo1hw4ZFRUVFrF279jPnL1q0KE488cTYf//9o7y8PK6//vr45JNPerVhAACAvlBwDK1cuTJqampi/vz5sX79+pgwYUJUV1fHe++9t9P5y5cvj5tuuinmz58fr776ajz00EOxcuXKuPnmm/d48wAAAL1VcAwtXLgwrrjiipg9e3acfPLJsXTp0hg+fHg8/PDDO53//PPPx1lnnRWXXXZZjB07Ni644IKYMWPG515NAgAA6E8FxVB7e3usW7cuqqqqPj3AoEFRVVUVDQ0NO11z5plnxrp167rj580334ynnnoqLrrooj3YNgAAwJ4ZUsjkbdu2RUdHR5SVlfUYLysri9dee22nay677LLYtm1bnH322dHV1RX/+c9/4sorr/zMl8m1tbVFW1tb9/2WlpZCtgkAAPC5+v3T5NasWRN33XVX3HfffbF+/fp47LHH4sknn4w77rhjl2vq6uqitLS0+1ZeXt7f2wQAAJIp6urq6trdye3t7TF8+PBYtWpVTJs2rXt81qxZsX379vjtb3+7w5pzzjknzjjjjLjnnnu6x375y1/GD3/4w/joo49i0KAde2xnV4bKy8ujubk5SkpKdne7AADAPqalpSVKS0v7pA0KujI0dOjQmDRpUtTX13ePdXZ2Rn19fVRWVu50zccff7xD8AwePDgiInbVYcXFxVFSUtLjBgAA0JcKes9QRERNTU3MmjUrJk+eHKeffnosWrQoWltbY/bs2RERMXPmzBgzZkzU1dVFRMTUqVNj4cKFceqpp0ZFRUVs3Lgxbr311pg6dWp3FAEAAOxtBcfQ9OnTY+vWrTFv3rxobGyMiRMnxurVq7s/VGHz5s09rgTdcsstUVRUFLfccku88847cdhhh8XUqVPjzjvv7LtHAQAAUKCC3jM0UPrydYEAAMAX14C9ZwgAAGBfIYYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUehVDS5YsibFjx8awYcOioqIi1q5d+5nzt2/fHldffXUcfvjhUVxcHCeccEI89dRTvdowAABAXxhS6IKVK1dGTU1NLF26NCoqKmLRokVRXV0dr7/+eowcOXKH+e3t7XH++efHyJEjY9WqVTFmzJj4+9//HgcddFBf7B8AAKBXirq6uroKWVBRURGnnXZaLF68OCIiOjs7o7y8PK655pq46aabdpi/dOnSuOeee+K1116L/fbbr1ebbGlpidLS0mhubo6SkpJeHQMAAPji68s2KOhlcu3t7bFu3bqoqqr69ACDBkVVVVU0NDTsdM3vfve7qKysjKuvvjrKyspi3Lhxcdddd0VHR8cuz9PW1hYtLS09bgAAAH2poBjatm1bdHR0RFlZWY/xsrKyaGxs3OmaN998M1atWhUdHR3x1FNPxa233hr33ntv/PjHP97leerq6qK0tLT7Vl5eXsg2AQAAPle/f5pcZ2dnjBw5Mh544IGYNGlSTJ8+PX70ox/F0qVLd7mmtrY2mpubu29btmzp720CAADJFPQBCiNGjIjBgwdHU1NTj/GmpqYYNWrUTtccfvjhsd9++8XgwYO7x770pS9FY2NjtLe3x9ChQ3dYU1xcHMXFxYVsDQAAoCAFXRkaOnRoTJo0Kerr67vHOjs7o76+PiorK3e65qyzzoqNGzdGZ2dn99gbb7wRhx9++E5DCAAAYG8o+GVyNTU1sWzZsnj00Ufj1VdfjauuuipaW1tj9uzZERExc+bMqK2t7Z5/1VVXxQcffBDXXnttvPHGG/Hkk0/GXXfdFVdffXXfPQoAAIACFfw9Q9OnT4+tW7fGvHnzorGxMSZOnBirV6/u/lCFzZs3x6BBnzZWeXl5PP3003H99dfH+PHjY8yYMXHttdfG3Llz++5RAAAAFKjg7xkaCL5nCAAAiBjA7xkCAADYV4ghAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKfUqhpYsWRJjx46NYcOGRUVFRaxdu3a31q1YsSKKiopi2rRpvTktAABAnyk4hlauXBk1NTUxf/78WL9+fUyYMCGqq6vjvffe+8x1b7/9dtxwww1xzjnn9HqzAAAAfaXgGFq4cGFcccUVMXv27Dj55JNj6dKlMXz48Hj44Yd3uaajoyMuv/zyWLBgQRxzzDF7tGEAAIC+UFAMtbe3x7p166KqqurTAwwaFFVVVdHQ0LDLdbfffnuMHDky5syZs1vnaWtri5aWlh43AACAvlRQDG3bti06OjqirKysx3hZWVk0NjbudM1f/vKXeOihh2LZsmW7fZ66urooLS3tvpWXlxeyTQAAgM/Vr58m9+GHH8Z3v/vdWLZsWYwYMWK319XW1kZzc3P3bcuWLf24SwAAIKMhhUweMWJEDB48OJqamnqMNzU1xahRo3aYv2nTpnj77bdj6tSp3WOdnZ3/PfGQIfH666/Hscceu8O64uLiKC4uLmRrAAAABSnoytDQoUNj0qRJUV9f3z3W2dkZ9fX1UVlZucP8k046KV5++eXYsGFD9+3rX/96nHvuubFhwwYvfwMAAAZMQVeGIiJqampi1qxZMXny5Dj99NNj0aJF0draGrNnz46IiJkzZ8aYMWOirq4uhg0bFuPGjeux/qCDDoqI2GEcAABgbyo4hqZPnx5bt26NefPmRWNjY0ycODFWr17d/aEKmzdvjkGD+vWtSAAAAHusqKurq2ugN/F5WlpaorS0NJqbm6OkpGSgtwMAAAyQvmwDl3AAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJCSGAIAAFISQwAAQEpiCAAASEkMAQAAKYkhAAAgJTEEAACkJIYAAICUxBAAAJBSr2JoyZIlMXbs2Bg2bFhUVFTE2rVrdzl32bJlcc4558TBBx8cBx98cFRVVX3mfAAAgL2h4BhauXJl1NTUxPz582P9+vUxYcKEqK6ujvfee2+n89esWRMzZsyIP/3pT9HQ0BDl5eVxwQUXxDvvvLPHmwcAAOitoq6urq5CFlRUVMRpp50WixcvjoiIzs7OKC8vj2uuuSZuuummz13f0dERBx98cCxevDhmzpy5W+dsaWmJ0tLSaG5ujpKSkkK2CwAA7EP6sg0KujLU3t4e69ati6qqqk8PMGhQVFVVRUNDw24d4+OPP45///vfccghh+xyTltbW7S0tPS4AQAA9KWCYmjbtm3R0dERZWVlPcbLysqisbFxt44xd+7cGD16dI+g+v/V1dVFaWlp9628vLyQbQIAAHyuvfppcnfffXesWLEiHn/88Rg2bNgu59XW1kZzc3P3bcuWLXtxlwAAQAZDCpk8YsSIGDx4cDQ1NfUYb2pqilGjRn3m2p/97Gdx9913x7PPPhvjx4//zLnFxcVRXFxcyNYAAAAKUtCVoaFDh8akSZOivr6+e6yzszPq6+ujsrJyl+t++tOfxh133BGrV6+OyZMn9363AAAAfaSgK0MRETU1NTFr1qyYPHlynH766bFo0aJobW2N2bNnR0TEzJkzY8yYMVFXVxcRET/5yU9i3rx5sXz58hg7dmz3e4sOOOCAOOCAA/rwoQAAAOy+gmNo+vTpsXXr1pg3b140NjbGxIkTY/Xq1d0fqrB58+YYNOjTC073339/tLe3x7e//e0ex5k/f37cdttte7Z7AACAXir4e4YGgu8ZAgAAIgbwe4YAAAD2FWIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASr2KoSVLlsTYsWNj2LBhUVFREWvXrv3M+b/5zW/ipJNOimHDhsWXv/zleOqpp3q1WQAAgL5ScAytXLkyampqYv78+bF+/fqYMGFCVFdXx3vvvbfT+c8//3zMmDEj5syZEy+++GJMmzYtpk2bFq+88soebx4AAKC3irq6uroKWVBRURGnnXZaLF68OCIiOjs7o7y8PK655pq46aabdpg/ffr0aG1tjd///vfdY2eccUZMnDgxli5dulvnbGlpidLS0mhubo6SkpJCtgsAAOxD+rINhhQyub29PdatWxe1tbXdY4MGDYqqqqpoaGjY6ZqGhoaoqanpMVZdXR1PPPHELs/T1tYWbW1t3febm5sj4r8PHAAAyOv/NkGB13R2qqAY2rZtW3R0dERZWVmP8bKysnjttdd2uqaxsXGn8xsbG3d5nrq6uliwYMEO4+Xl5YVsFwAA2Ee9//77UVpaukfHKCiG9pba2toeV5M6Ozvjgw8+iEMPPTSKior67DwtLS1RXl4eW7Zs8fK7fuDvF/Y+zzsA9nXNzc1x5JFHxiGHHLLHxyoohkaMGBGDBw+OpqamHuNNTU0xatSona4ZNWpUQfMjIoqLi6O4uLjH2EEHHVTIVgtSUlLiPw39yN8v7H2edwDs6wYN2vNvCSroCEOHDo1JkyZFfX1991hnZ2fU19dHZWXlTtdUVlb2mB8R8cwzz+xyPgAAwN5Q8MvkampqYtasWTF58uQ4/fTTY9GiRdHa2hqzZ8+OiIiZM2fGmDFjoq6uLiIirr322vif//mfuPfee+Piiy+OFStWxAsvvBAPPPBA3z4SAACAAhQcQ9OnT4+tW7fGvHnzorGxMSZOnBirV6/u/pCEzZs397hkdeaZZ8by5cvjlltuiZtvvjmOP/74eOKJJ2LcuHF99yh6qbi4OObPn7/DS/LoG/5+Ye/zvANgX9eXv+sK/p4hAACAfcGev+sIAADgC0gMAQAAKYkhAAAgJTEEAACklDKGnnvuuZg6dWqMHj06ioqK4oknnhjoLe1TbrvttigqKupxO+mkkwZ6W7BP+byfY9/73vd2eB5eeOGFA7NZACjQ/fffH+PHj+/+EvHKysr4wx/+0P3nX/va13b4PXfllVcWfJ6CP1p7X9Da2hoTJkyI73//+3HJJZcM9Hb2Saeccko8++yz3feHDEn5Tw36ze78HLvwwgvjF7/4Rfd9H7cNwBfFEUccEXfffXccf/zx0dXVFY8++mh84xvfiBdffDFOOeWUiIi44oor4vbbb+9eM3z48ILPk/J/qFOmTIkpU6YM9Db2aUOGDIlRo0YN9DZgn7U7P8eKi4s9DwH4Qpo6dWqP+3feeWfcf//98de//rU7hoYPH77Hv+dSvkyO/ve3v/0tRo8eHcccc0xcfvnlsXnz5oHeEqSzZs2aGDlyZJx44olx1VVXxfvvvz/QWwKAgnV0dMSKFSuitbU1Kisru8d/9atfxYgRI2LcuHFRW1sbH3/8ccHHTnlliP5VUVERjzzySJx44onx7rvvxoIFC+Kcc86JV155JQ488MCB3h6kcOGFF8Yll1wSRx99dGzatCluvvnmmDJlSjQ0NMTgwYMHensA8LlefvnlqKysjE8++SQOOOCAePzxx+Pkk0+OiIjLLrssjjrqqBg9enS89NJLMXfu3Hj99dfjscceK+gcRV1dXV39sfkviqKionj88cdj2rRpA72Vfdb27dvjqKOOioULF8acOXMGejuwz9mdn2NvvvlmHHvssfHss8/Geeedt/c2BwC91N7eHps3b47m5uZYtWpVPPjgg/HnP/+5O4j+X3/84x/jvPPOi40bN8axxx672+fwMjn63UEHHRQnnHBCbNy4caC3Amkdc8wxMWLECM9DAL4whg4dGscdd1xMmjQp6urqYsKECfHzn/98p3MrKioiIgr+PSeG6HcfffRRbNq0KQ4//PCB3gqk9Y9//CPef/99z0MAvrA6Ozujra1tp3+2YcOGiIiCf8+lfM/QRx991KMa33rrrdiwYUMccsghceSRRw7gzvYNN9xwQ0ydOjWOOuqo+Oc//xnz58+PwYMHx4wZMwZ6a7DP+KyfY4ccckgsWLAgvvWtb8WoUaNi06ZNceONN8Zxxx0X1dXVA7hrANg9tbW1MWXKlDjyyCPjww8/jOXLl8eaNWvi6aefjk2bNsXy5cvjoosuikMPPTReeumluP766+OrX/1qjB8/vqDzpHzP0Jo1a+Lcc8/dYXzWrFnxyCOP7P0N7WMuvfTSeO655+L999+Pww47LM4+++y48847C3r9JvDZPuvn2P333x/Tpk2LF198MbZv3x6jR4+OCy64IO64444oKysbgN0CQGHmzJkT9fX18e6770ZpaWmMHz8+5s6dG+eff35s2bIlvvOd78Qrr7wSra2tUV5eHt/85jfjlltuiZKSkoLOkzKGAAAAvGcIAABISQwBAAApiSEAACAlMQQAAKQkhgAAgJTEEAAAkJIYAgAAUhJDAABASmIIAABISQwBAAApiSEAACAlMQQAAKT0fwAGGFyeaNPd5QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-x1u5NpnDNG"
      },
      "source": [
        "## LR = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "kvals = [1, 5, 15, 35]\n",
        "pvals = [1.0]\n",
        "epochs = 80\n",
        "for p in pvals:\n",
        "    for k in kvals:\n",
        "         train_acc, test_acc, loss_hist =  train_model_k_p(X_train, y_train, X_test, y_test, p, k, epochs, lr = 0.01)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xticks(kvals)\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Train Accuracy')\n",
        "    plt.plot(kvals, [test_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Test Accuracy')\n",
        "    plt.xlabel('Number of Neurons (k)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Accuracy vs Number of Neurons (p={p})')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    print(f\"\\nFinal train accuracy k = {k}, p = {p}: {train_acc[(k, p, epochs-1)]}\")\n",
        "    print(f\"\\nFinal test accuracy k = {k}, p = {p}: {test_acc[(k, p, epochs-1)]}\")"
      ],
      "metadata": {
        "id": "1ZayZAOvqHat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2\n",
        "\n",
        "1. Generally the training accuracy has decreased with increasing p which makes sense as the more neurons are randomly deactivated, the network's capacity to learning data lessens. It dropped a lot in p = 1.0 case where in all neurons were deactivated, then the model didnt learn anything.\n",
        "\n",
        "2. When p is smaller its generally more difficult to optimize than when p is higher because less number of gradients have to be calculated and backpropagated in this case than higher p."
      ],
      "metadata": {
        "id": "M8KuDyc6tQYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvals = [1, 5, 15, 35]\n",
        "pvals = [0.1, 0.25, 0.5, 1.0]\n",
        "epochs = 80\n",
        "torch.manual_seed(0)\n",
        "for p in pvals:\n",
        "    for k in kvals:\n",
        "         train_acc, test_acc, loss_hist =  train_model_k_p(X_train, y_train, X_test, y_test, p, k, epochs, lr = 0.01)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xticks(kvals)\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Train Accuracy')\n",
        "    plt.plot(kvals, [test_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Test Accuracy')\n",
        "    plt.xlabel(' of Neurons (k)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Accuracy vs Number of Neurons (p={p})')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    print(f\"\\nFinal train accuracy k = {k}, p = {p}: {train_acc[(k, p, epochs-1)]}\")\n",
        "    print(f\"\\nFinal test accuracy k = {k}, p = {p}: {test_acc[(k, p, epochs-1)]}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for p in pvals:\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o',label=f\"p = {p}\")\n",
        "plt.xticks(kvals)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title('Training Accuracy vs # Hidden Units')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rSBscMRgp5ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently, none of the networks have reached training accuracy exactly 100% though k = 35, p = 0.1 is very close. In order to determine at what min k among given values will it be achieved, I have to increase the number of epochs."
      ],
      "metadata": {
        "id": "Orff86dA_vl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2\n",
        "\n",
        "3. For all values of p (except P = 1.0), a training accuracy of 100% is achieved at k = 35, given sufficient epochs to train.\n",
        "For p = 1.0, a 100% accuracy will never be achieved as the model won't ever be able to learn the training data."
      ],
      "metadata": {
        "id": "mWZDCAnCAZ6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kvals = [1, 5, 15, 35]\n",
        "pvals = [0.1, 0.25, 0.5, 1.0]\n",
        "epochs = 300\n",
        "torch.manual_seed(0)\n",
        "for p in pvals:\n",
        "    for k in kvals:\n",
        "         train_acc, test_acc, loss_hist =  train_model_k_p(X_train, y_train, X_test, y_test, p, k, epochs, lr = 0.01)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xticks(kvals)\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Train Accuracy')\n",
        "    plt.plot(kvals, [test_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Test Accuracy')\n",
        "    plt.xlabel('Number of Neurons (k)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Accuracy vs Number of Neurons (p={p})')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    print(f\"\\nFinal train accuracy k = {k}, p = {p}: {train_acc[(k, p, epochs-1)]}\")\n",
        "    print(f\"\\nFinal test accuracy k = {k}, p = {p}: {test_acc[(k, p, epochs-1)]}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for p in pvals:\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o',label=f\"p = {p}\")\n",
        "plt.xticks(kvals)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.title('Training Accuracy vs # Hidden Units')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1HJk1gCQAuI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3\n",
        "\n",
        "1. Yes, generally higher the dropout (1-p) or lower the p, the test accuracy is higher (except in the case of p = 0) , it makes sense because the model is more generalizable with more dropout.\n",
        "\n",
        "2. For k = 35, and p = 0.0, i get highest test accuracy (i.e. no dropout case)"
      ],
      "metadata": {
        "id": "IbXzobx3AVQl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18378098"
      },
      "source": [
        "kvals = [1, 5, 15, 35]\n",
        "pvals = [0.1, 0.25, 0.5, 1.0]\n",
        "epochs = 80\n",
        "torch.manual_seed(0)\n",
        "for p in pvals:\n",
        "    for k in kvals:\n",
        "         train_acc, test_acc, loss_hist =  train_model_k_p(X_train, y_train, X_test, y_test, p, k, epochs, lr = 0.01)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xticks(kvals)\n",
        "    plt.plot(kvals, [train_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Train Accuracy')\n",
        "    plt.plot(kvals, [test_acc[(k, p, epochs-1)] for k in kvals], marker='o', label='Test Accuracy')\n",
        "    plt.xlabel('Number of Neurons (k)')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title(f'Accuracy vs Number of Neurons (p={p})')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    print(f\"\\nFinal train accuracy k = {k}, p = {p}: {train_acc[(k, p, epochs-1)]}\")\n",
        "    print(f\"\\nFinal test accuracy k = {k}, p = {p}: {test_acc[(k, p, epochs-1)]}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for p in pvals:\n",
        "    plt.plot(kvals, [test_acc[(k, p, epochs-1)] for k in kvals], marker='o',label=f\"p = {p}\")\n",
        "plt.xticks(kvals)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Trest Accuracy vs # Hidden Units')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}