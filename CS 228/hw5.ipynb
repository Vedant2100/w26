{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0gLuOTo9G2Q"
   },
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HGPT9YcIR7aZ",
    "outputId": "47ade6ad-ccb6-4231-ab17-7b4470f667e7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "# Extract data if zip exists and folder doesn't\n",
    "if os.path.exists('data.zip') and not os.path.exists('data/names'):\n",
    "    print('Extracting data.zip...')\n",
    "    os.system('unzip -q data.zip')\n",
    "\n",
    "# Download PTB if missing\n",
    "ptb_path = 'data/ptb'\n",
    "os.makedirs(ptb_path, exist_ok=True)\n",
    "ptb_files = {\n",
    "    'ptb.train.txt': 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.train.txt',\n",
    "    'ptb.valid.txt': 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.valid.txt',\n",
    "    'ptb.test.txt': 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.test.txt'\n",
    "}\n",
    "\n",
    "for filename, url in ptb_files.items():\n",
    "    dest = os.path.join(ptb_path, filename)\n",
    "    if not os.path.exists(dest):\n",
    "        print(f'Downloading {filename}...')\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JqdvTlB0djx"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
    "n_letters = len(allowed_characters)\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhROWoYa05Fj"
   },
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    if letter not in allowed_characters:\n",
    "        return allowed_characters.find(\"_\")\n",
    "    else:\n",
    "        return allowed_characters.find(letter)\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "qhUxBfj52NUx",
    "outputId": "0980ccaa-e27c-4d03-9f1d-1b3850321e3c"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, data_dir=None, file_path=None, delimiter=None, label_col=0, line_col=1):\n",
    "        import glob, os, time\n",
    "        self.load_time = time.localtime\n",
    "        labels_set = set()\n",
    "        self.data = []\n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "        if file_path:\n",
    "            import csv\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                try:\n",
    "                    sample = f.read(1024)\n",
    "                    f.seek(0)\n",
    "                    dialect = csv.Sniffer().sniff(sample)\n",
    "                    delimiter = dialect.delimiter\n",
    "                except:\n",
    "                    delimiter = delimiter or ','\n",
    "                reader = csv.reader(f, delimiter=delimiter)\n",
    "                for row in reader:\n",
    "                    if len(row) > max(label_col, line_col):\n",
    "                        label = row[label_col]\n",
    "                        name = row[line_col]\n",
    "                        labels_set.add(label)\n",
    "                        self.data.append(name)\n",
    "                        self.data_tensors.append(lineToTensor(name))\n",
    "                        self.labels.append(label)\n",
    "        elif data_dir:\n",
    "            text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "            for filename in text_files:\n",
    "                label = os.path.splitext(os.path.basename(filename))[0]\n",
    "                labels_set.add(label)\n",
    "                lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "                for name in lines:\n",
    "                    self.data.append(name)\n",
    "                    self.data_tensors.append(lineToTensor(name))\n",
    "                    self.labels.append(label)\n",
    "        else:\n",
    "            raise ValueError(\"Either data_dir or file_path must be provided\")\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for idx in range(len(self.labels)):\n",
    "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.labels_tensors[idx], self.data_tensors[idx], self.labels[idx], self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eIB2L0C2YMQ"
   },
   "outputs": [],
   "source": [
    "alldata = NamesDataset(data_dir=\"data/names\")\n",
    "print(f\"loaded {len(alldata)} items of data\")\n",
    "print(f\"example = {alldata[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "vyDK57z029Yp",
    "outputId": "0f81497f-5696-417e-9a94-7804919c7c1a"
   },
   "outputs": [],
   "source": [
    "def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    all_grad_norms = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "    start = time.time()\n",
    "    print(f\"Loaded: {len(training_data)}\")\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, max(1, len(batches) // n_batch_size))\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            for i in batch:\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                output = rnn.forward(text_tensor)\n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss / len(batch)\n",
    "            batch_loss.backward()\n",
    "            norm = nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            all_grad_norms.append(norm.item() if hasattr(norm, 'item') else norm)\n",
    "            optimizer.step()\n",
    "            current_loss += batch_loss.item()\n",
    "        all_losses.append(current_loss / len(batches))\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t Loss: {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "    return all_losses, all_grad_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t0rv_H73Pg0"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rnn_type='RNN', n_layers=1, extra_layers=0):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, n_layers)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, n_layers)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, n_layers)\n",
    "        self.linears = nn.ModuleList()\n",
    "        curr_size = hidden_size\n",
    "        for _ in range(extra_layers):\n",
    "            self.linears.append(nn.Linear(curr_size, curr_size))\n",
    "            self.linears.append(nn.ReLU())\n",
    "        self.h2o = nn.Linear(curr_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, line_tensor):\n",
    "        rnn_out, hidden = self.rnn(line_tensor)\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            output = hidden[0][-1]\n",
    "        else:\n",
    "            output = hidden[-1]\n",
    "        for layer in self.linears:\n",
    "            output = layer(output)\n",
    "        output = self.h2o(output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "WXQ1PAub3vsN",
    "outputId": "fb1d6173-7b65-4ffe-e65b-5832130d05d8"
   },
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQJGwugB31Mg"
   },
   "outputs": [],
   "source": [
    "def label_from_output(output, output_labels):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    label_i = top_i[0].item()\n",
    "    return output_labels[label_i], label_i\n",
    "input = lineToTensor('Albert')\n",
    "output = rnn(input)\n",
    "print(output)\n",
    "print(label_from_output(output, alldata.labels_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "4lE6RBgz34yq",
    "outputId": "f426e7f5-2621-42f9-90b1-68f8df837252"
   },
   "outputs": [],
   "source": [
    "def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    rnn.train()\n",
    "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "    start = time.time()\n",
    "    print(f\"Loaded: {len(training_data)}\")\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        rnn.zero_grad()\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch:\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                output = rnn.forward(text_tensor)\n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t Loss: {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kwl5XLV74h3I"
   },
   "source": [
    "tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNexykXg4daw"
   },
   "outputs": [],
   "source": [
    "def run_experiment_with_data(dataset, rnn_type='RNN', n_hidden=128, n_layers=1, extra_layers=0, n_epoch=10, learning_rate=0.2, n_batch_size=64):\n",
    "    import time\n",
    "    train_s, test_s = torch.utils.data.random_split(dataset, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
    "    print(f'\\n--- Experiment: {rnn_type}, hidden={n_hidden}, layers={n_layers} ---')\n",
    "    model = CharRNN(n_letters, n_hidden, len(dataset.labels_uniq), rnn_type=rnn_type, n_layers=n_layers, extra_layers=extra_layers)\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    losses, grads = train(model, train_s, n_epoch=n_epoch, n_batch_size=n_batch_size, learning_rate=learning_rate, report_every=max(1, n_epoch//5))\n",
    "    print(f'Done in {time.time() - start_time:.2f}s')\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(test_s)):\n",
    "            (label_tensor, text_tensor, label, text) = test_s[i]\n",
    "            output = model(text_tensor)\n",
    "            guess, guess_i = label_from_output(output, dataset.labels_uniq)\n",
    "            if guess == label: correct += 1\n",
    "    print(f'Accuracy: {correct/len(test_s):.4f}')\n",
    "    # Plotting results for immediate feedback\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(losses); ax1.set_title('Loss'); ax1.set_xlabel('Epoch')\n",
    "    ax2.plot(grads); ax2.set_title('Grad Norms'); ax2.set_xlabel('Iteration')\n",
    "    plt.show()\n",
    "    return model, losses, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "alldata = NamesDataset(data_dir=\"data/names\")\n",
    "rnn_model, rnn_losses, rnn_grads = run_experiment_with_data(alldata, rnn_type='RNN', n_epoch=20)\n",
    "def run_experiment_with_grads(dataset, rnn_type='RNN', n_hidden=128, n_layers=1, n_epoch=10):\n",
    "    train_s, test_s = torch.utils.data.random_split(dataset, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
    "    model = CharRNN(n_letters, n_hidden, len(dataset.labels_uniq), rnn_type=rnn_type, n_layers=n_layers)\n",
    "    model.to(device)\n",
    "    losses, grads = train(model, train_s, n_epoch=n_epoch)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(losses)\n",
    "    plt.title('RNN Learning Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(grads)\n",
    "    plt.title('RNN Gradient Norms')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Norm')\n",
    "    plt.show()\n",
    "    return model, losses, grads\n",
    "rnn_model, rnn_losses, rnn_grads = run_experiment_with_grads(alldata, n_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-k2lSwuS4lMd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "0t8WNvNN4psG",
    "outputId": "b96b0d96-aec7-41bd-91cb-c4f5ad0fa7d3"
   },
   "outputs": [],
   "source": [
    "def evaluate(rnn, testing_data, classes):\n",
    "    confusion = torch.zeros(len(classes), len(classes))\n",
    "    rnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(testing_data)):\n",
    "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
    "            output = rnn(text_tensor)\n",
    "            guess, guess_i = label_from_output(output, classes)\n",
    "            label_i = classes.index(label)\n",
    "            confusion[label_i][guess_i] += 1\n",
    "    for i in range(len(classes)):\n",
    "        denom = confusion[i].sum()\n",
    "        if denom > 0:\n",
    "            confusion[i] = confusion[i] / denom\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.cpu().numpy())\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_with_data(alldata, rnn_type='GRU', n_layers=2, n_hidden = 256, n_epoch = 27)\n",
    "run_experiment_with_data(alldata, rnn_type='GRU', n_layers=2, n_hidden = 128, n_epoch = 27)\n",
    "run_experiment_with_data(alldata, rnn_type='LSTM', n_layers=2,n_hidden = 128, n_epoch = 27)\n",
    "reddit_data = NamesDataset(file_path=\"reddit.tsv\", delimiter=\"\\t\", label_col=1, line_col=0)\n",
    "run_experiment_with_data(reddit_data, rnn_type='GRU', n_epoch=20, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Language Modeling with PTB\n",
    "Zaremba model experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBDataset:\n",
    "    def __init__(self, file_path, batch_size, num_steps, vocab=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.words = open(file_path).read().replace('\\n', '<eos>').split()\n",
    "        if vocab is None:\n",
    "            self.vocab = sorted(list(set(self.words)))\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "        self.word_to_id = {word: i for i, word in enumerate(self.vocab)}\n",
    "        self.id_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
    "        # Handle OOV by assigning to a special token index if we were being rigorous, \n",
    "        # but for PTB it's usually closed vocab or <unk> is already there.\n",
    "        self.data = np.array([self.word_to_id.get(word, 0) for word in self.words], dtype=np.int64)\n",
    "        num_batches = len(self.data) // batch_size\n",
    "        self.data = self.data[:num_batches * batch_size]\n",
    "        self.data = self.data.reshape((batch_size, num_batches))\n",
    "    def __iter__(self):\n",
    "        for i in range(0, self.data.shape[1] - self.num_steps, self.num_steps):\n",
    "            x = self.data[:, i:i+self.num_steps]\n",
    "            y = self.data[:, i+1:i+self.num_steps+1]\n",
    "            yield torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "    def __len__(self):\n",
    "        return (self.data.shape[1] - 1) // self.num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZarembaModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.5, variational=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.variational = variational\n",
    "    def forward(self, x, hidden=None):\n",
    "        embeds = self.dropout(self.embedding(x))\n",
    "        output, hidden = self.lstm(embeds, hidden)\n",
    "        output = self.dropout(output)\n",
    "        logits = self.linear(output.reshape(-1, self.hidden_size))\n",
    "        return logits, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                weight.new_zeros(self.num_layers, batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.mask = None\n",
    "    def reset_mask(self, x):\n",
    "        self.mask = torch.bernoulli(torch.full((x.size(0), 1, x.size(2)), 1 - self.dropout)).to(x.device) / (1 - self.dropout)\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.dropout == 0:\n",
    "            return x\n",
    "        if self.mask is None or self.mask.size(0) != x.size(0):\n",
    "            self.reset_mask(x)\n",
    "        return x * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalZarembaModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstms = nn.ModuleList([nn.LSTM(hidden_size, hidden_size, 1, batch_first=True) for _ in range(num_layers)])\n",
    "        self.dropouts = nn.ModuleList([VariationalDropout(dropout) for _ in range(num_layers + 1)])\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, x, hiddens=None):\n",
    "        if hiddens is None:\n",
    "            hiddens = [None] * self.num_layers\n",
    "        if self.training:\n",
    "            pass\n",
    "        x = self.dropouts[0](self.embedding(x))\n",
    "        new_hiddens = []\n",
    "        for i, lstm in enumerate(self.lstms):\n",
    "            x, h = lstm(x, hiddens[i])\n",
    "            x = self.dropouts[i+1](x)\n",
    "            new_hiddens.append(h)\n",
    "        logits = self.linear(x.reshape(-1, self.hidden_size))\n",
    "        return logits, new_hiddens\n",
    "    def reset_dropout_masks(self, batch_size, device):\n",
    "        dummy_input = torch.zeros(batch_size, 1, self.hidden_size).to(device)\n",
    "        for d in self.dropouts:\n",
    "            d.reset_mask(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ptb(model, train_data, valid_data, n_epochs=13, lr=1.0, lr_decay=0.5, decay_after=4, variational=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    all_train_ppl = []\n",
    "    all_valid_ppl = []\n",
    "    grad_norms = []\n",
    "    best_valid_ppl = float('inf')\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch > decay_after:\n",
    "            lr *= lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        hidden = None\n",
    "        num_batches = 0\n",
    "        for x, y in train_data:\n",
    "            if variational:\n",
    "                model.reset_dropout_masks(x.size(0), x.device)\n",
    "            optimizer.zero_grad()\n",
    "            if hidden is not None:\n",
    "                if isinstance(hidden, list):\n",
    "                    hidden = [(h[0].detach(), h[1].detach()) for h in hidden]\n",
    "                else:\n",
    "                    hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = criterion(output, y.reshape(-1))\n",
    "            loss.backward()\n",
    "            norm = nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            grad_norms.append(norm.item() if hasattr(norm, 'item') else norm)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        train_ppl = np.exp(total_loss / num_batches)\n",
    "        all_train_ppl.append(train_ppl)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            h_val = None\n",
    "            for x_v, y_v in valid_data:\n",
    "                out_v, h_val = model(x_v, h_val)\n",
    "                loss_v = criterion(out_v, y_v.reshape(-1))\n",
    "                val_loss += loss_v.item()\n",
    "                val_batches += 1\n",
    "        valid_ppl = np.exp(val_loss / val_batches)\n",
    "        all_valid_ppl.append(valid_ppl)\n",
    "        print(f'Epoch {epoch+1}: Train PPL {train_ppl:.2f}, Valid PPL {valid_ppl:.2f}')\n",
    "    return all_train_ppl, all_valid_ppl, grad_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiments (PTB)\n",
    "Training baseline and variational models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_steps = 20\n",
    "train_data = PTBDataset('data/ptb/ptb.train.txt', batch_size, num_steps)\n",
    "valid_data = PTBDataset('data/ptb/ptb.valid.txt', batch_size, num_steps, vocab=train_data.vocab)\n",
    "test_data = PTBDataset('data/ptb/ptb.test.txt', batch_size, num_steps, vocab=train_data.vocab)\n",
    "vocab_size = len(train_data.vocab)\n",
    "hidden_size = 200\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting baseline...\")\n",
    "model_std = ZarembaModel(vocab_size, hidden_size, num_layers).to(device)\n",
    "import time\n",
    "start_time = time.time()\n",
    "train_ppls_std, valid_ppls_std, grad_norms_std = train_ptb(model_std, train_data, valid_data, n_epochs=13, variational=False)\n",
    "print(f\"Std time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStarting variational...\")\n",
    "model_var = VariationalZarembaModel(vocab_size, hidden_size, num_layers).to(device)\n",
    "start_time = time.time()\n",
    "train_ppls_var, valid_ppls_var, grad_norms_var = train_ptb(model_var, train_data, valid_data, n_epochs=13, variational=True)\n",
    "print(f\"Var time: {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_ppl(std_train, std_val, var_train, var_val):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(std_train, label='Std Train PPL', linestyle='--')\n",
    "    plt.plot(std_val, label='Std Valid PPL')\n",
    "    plt.plot(var_train, label='Var Train PPL', linestyle='--')\n",
    "    plt.plot(var_val, label='Var Valid PPL')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Training and Validation Learning Curve (Perplexity)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "plot_ppl(train_ppls_std, valid_ppls_std, train_ppls_var, valid_ppls_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grads(std_grads, var_grads):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(std_grads, label='Std Gradient Norm', alpha=0.5)\n",
    "    plt.plot(var_grads, label='Var Gradient Norm', alpha=0.5)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('L2 Norm of Gradient')\n",
    "    plt.title('L2 Norm of Minibatch Gradient per Iteration')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_grads(grad_norms_std, grad_norms_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bb8PqLeFB6md"
   },
   "outputs": [],
   "source": [
    "run_experiment_with_data(alldata, rnn_type='GRU', n_layers=2, n_hidden = 256, n_epoch = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Adxy1DYBNR4"
   },
   "outputs": [],
   "source": [
    "run_experiment_with_data(alldata, rnn_type='GRU', n_layers=2, n_hidden = 128, n_epoch = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "ecz21sOFBkfq",
    "outputId": "a24ddcdd-2e56-44f9-ccaf-505ae231c46c"
   },
   "outputs": [],
   "source": [
    "run_experiment_with_data(alldata, rnn_type='LSTM', n_layers=2,n_hidden = 128, n_epoch = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2AC6EeUCAso"
   },
   "outputs": [],
   "source": [
    "reddit_data = NamesDataset(file_path=\"reddit.tsv\", delimiter=\"\\t\", label_col=1, line_col=0)\n",
    "run_experiment_with_data(reddit_data, rnn_type='GRU', n_epoch=20, learning_rate=0.1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "G4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}