{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0gLuOTo9G2Q"
      },
      "source": [
        "# Q1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGPT9YcIR7aZ",
        "outputId": "fe414083-b947-4633-83ca-0e7cf9297aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device = cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "\n",
        "torch.set_default_device(device)\n",
        "print(f\"Using device = {torch.get_default_device()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uhREtOv9Tv8G"
      },
      "outputs": [],
      "source": [
        "# ! unzip data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-JqdvTlB0djx"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import unicodedata\n",
        "\n",
        "# We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
        "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
        "n_letters = len(allowed_characters)\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in allowed_characters\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KhROWoYa05Fj"
      },
      "outputs": [],
      "source": [
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    # return our out-of-vocabulary character if we encounter a letter unknown to our model\n",
        "    if letter not in allowed_characters:\n",
        "        return allowed_characters.find(\"_\")\n",
        "    else:\n",
        "        return allowed_characters.find(letter)\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def lineToTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li, letter in enumerate(line):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qhUxBfj52NUx",
        "outputId": "d3deaa4e-f974-4643-d9ab-5b145a02c74c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unexpected character after line continuation character (575941365.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2672/575941365.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class NamesDataset(Dataset):\\n\\n    def __init__(self, data_dir=None, file_path=None, delimiter=None, label_col=0, line_col=1):\\n        # Generalizing to handle both directory of files and single delimited file\\n        import glob, os, time\\n        self.load_time = time.localtime\\n        labels_set = set()\\n\\n        self.data = []\\n        self.data_tensors = []\\n        self.labels = []\\n        self.labels_tensors = []\\n\\n        if file_path:\\n            # Single file mode (CSV/TSV/etc)\\n            import csv\\n            with open(file_path, encoding='utf-8') as f:\\n                # Auto-detect delimiter if not provided\\n                try:\\n                    sample = f.read(1024)\\n                    f.seek(0)\\n                    dialect = csv.Sniffer().sniff(sample)\\n                    delimiter = dialect.delimiter\\n                except:\\n                    delimiter = delimiter or ','\\n                \\n                reader = csv.reader(f, delimiter=delimiter)\\n                for row in reader:\\n                    if len(row) > max(label_col, line_col):\\n                        label = row[label_col]\\n                        name = row[line_col]\\n                        labels_set.add(label)\\n                        self.data.append(name)\\n                        self.data_tensors.append(lineToTensor(name))\\n                        self.labels.append(label)\\n        elif data_dir:\\n            # Directory mode (Original behavior)\\n    ...\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ],
      "source": [
        "class NamesDataset(Dataset):\\n\\n    def __init__(self, data_dir=None, file_path=None, delimiter=None, label_col=0, line_col=1):\\n        # Generalizing to handle both directory of files and single delimited file\\n        import glob, os, time\\n        self.load_time = time.localtime\\n        labels_set = set()\\n\\n        self.data = []\\n        self.data_tensors = []\\n        self.labels = []\\n        self.labels_tensors = []\\n\\n        if file_path:\\n            # Single file mode (CSV/TSV/etc)\\n            import csv\\n            with open(file_path, encoding='utf-8') as f:\\n                # Auto-detect delimiter if not provided\\n                try:\\n                    sample = f.read(1024)\\n                    f.seek(0)\\n                    dialect = csv.Sniffer().sniff(sample)\\n                    delimiter = dialect.delimiter\\n                except:\\n                    delimiter = delimiter or ','\\n                \\n                reader = csv.reader(f, delimiter=delimiter)\\n                for row in reader:\\n                    if len(row) > max(label_col, line_col):\\n                        label = row[label_col]\\n                        name = row[line_col]\\n                        labels_set.add(label)\\n                        self.data.append(name)\\n                        self.data_tensors.append(lineToTensor(name))\\n                        self.labels.append(label)\\n        elif data_dir:\\n            # Directory mode (Original behavior)\\n            text_files = glob.glob(os.path.join(data_dir, '*.txt'))\\n            for filename in text_files:\\n                label = os.path.splitext(os.path.basename(filename))[0]\\n                labels_set.add(label)\\n                lines = open(filename, encoding='utf-8').read().strip().split('\\n')\\n                for name in lines:\\n                    self.data.append(name)\\n                    self.data_tensors.append(lineToTensor(name))\\n                    self.labels.append(label)\\n        else:\\n            raise ValueError(\"Either data_dir or file_path must be provided\")\\n\\n        # Cache the tensor representation of the labels\\n        self.labels_uniq = list(labels_set)\\n        for idx in range(len(self.labels)):\\n            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\\n            self.labels_tensors.append(temp_tensor)\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        return self.labels_tensors[idx], self.data_tensors[idx], self.labels[idx], self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eIB2L0C2YMQ"
      },
      "outputs": [],
      "source": [
        "alldata = NamesDataset(data_dir=\"data/names\")\\nprint(f\"loaded {len(alldata)} items of data\")\\nprint(f\"example = {alldata[0]}\")\\n\\n# To try with a different dataset (e.g., CSV with label in col 1 and text in col 0):\\n# other_data = NamesDataset(file_path=\"path/to/data.csv\", delimiter=\",\", label_col=1, line_col=0)\\n# run_experiment_with_data(other_data, rnn_type='LSTM')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyDK57z029Yp"
      },
      "outputs": [],
      "source": [
        "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
        "\n",
        "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t0rv_H73Pg0"
      },
      "outputs": [],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, rnn_type='RNN', n_layers=1, extra_layers=0):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        if rnn_type == 'LSTM':\n",
        "            self.rnn = nn.LSTM(input_size, hidden_size, n_layers)\n",
        "        elif rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(input_size, hidden_size, n_layers)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(input_size, hidden_size, n_layers)\n",
        "\n",
        "        self.linears = nn.ModuleList()\n",
        "        curr_size = hidden_size\n",
        "        for _ in range(extra_layers):\n",
        "            self.linears.append(nn.Linear(curr_size, curr_size))\n",
        "            self.linears.append(nn.ReLU())\n",
        "\n",
        "        self.h2o = nn.Linear(curr_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, line_tensor):\n",
        "        rnn_out, hidden = self.rnn(line_tensor)\n",
        "\n",
        "        # For LSTM/GRU/RNN, we take the last hidden state of the last layer\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            # hidden is (h_n, c_n), we want h_n\n",
        "            output = hidden[0][-1]\n",
        "        else:\n",
        "            # hidden is h_n\n",
        "            output = hidden[-1]\n",
        "\n",
        "        for layer in self.linears:\n",
        "            output = layer(output)\n",
        "\n",
        "        output = self.h2o(output)\n",
        "        output = self.softmax(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXQ1PAub3vsN"
      },
      "outputs": [],
      "source": [
        "n_hidden = 128\n",
        "rnn = CharRNN(n_letters, n_hidden, len(alldata.labels_uniq))\n",
        "print(rnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQJGwugB31Mg"
      },
      "outputs": [],
      "source": [
        "def label_from_output(output, output_labels):\n",
        "    top_n, top_i = output.topk(1)\n",
        "    label_i = top_i[0].item()\n",
        "    return output_labels[label_i], label_i\n",
        "\n",
        "input = lineToTensor('Albert')\n",
        "output = rnn(input) #this is equivalent to ``output = rnn.forward(input)``\n",
        "print(output)\n",
        "print(label_from_output(output, alldata.labels_uniq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lE6RBgz34yq"
      },
      "outputs": [],
      "source": [
        "def train(rnn, training_data, n_epoch = 10, n_batch_size = 64, report_every = 50, learning_rate = 0.2, criterion = nn.NLLLoss()):\n",
        "    \"\"\"\n",
        "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
        "    \"\"\"\n",
        "    # Keep track of losses for plotting\n",
        "    current_loss = 0\n",
        "    all_losses = []\n",
        "    rnn.train()\n",
        "    optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "    start = time.time()\n",
        "    print(f\"training on data set with n = {len(training_data)}\")\n",
        "\n",
        "    for iter in range(1, n_epoch + 1):\n",
        "        rnn.zero_grad() # clear the gradients\n",
        "\n",
        "        # create some minibatches\n",
        "        # we cannot use dataloaders because each of our names is a different length\n",
        "        batches = list(range(len(training_data)))\n",
        "        random.shuffle(batches)\n",
        "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
        "\n",
        "        for idx, batch in enumerate(batches):\n",
        "            batch_loss = 0\n",
        "            for i in batch: #for each example in this batch\n",
        "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
        "                output = rnn.forward(text_tensor)\n",
        "                loss = criterion(output, label_tensor)\n",
        "                batch_loss += loss\n",
        "\n",
        "            # optimize parameters\n",
        "            batch_loss.backward()\n",
        "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            current_loss += batch_loss.item() / len(batch)\n",
        "\n",
        "        all_losses.append(current_loss / len(batches) )\n",
        "        if iter % report_every == 0:\n",
        "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
        "        current_loss = 0\n",
        "\n",
        "    return all_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kwl5XLV74h3I"
      },
      "source": [
        "tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNexykXg4daw"
      },
      "outputs": [],
      "source": [
        "def run_experiment_with_data(dataset, rnn_type='RNN', n_hidden=128, n_layers=1, extra_layers=0, n_epoch=10, learning_rate=0.2, n_batch_size=64):\\n    # Helper to run experiments with specific datasets\\n    train_s, test_s = torch.utils.data.random_split(dataset, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\\n    \\n    print(f\"\\n--- Experiment with {len(dataset)} items ---\")\\n    print(f\"Architectue: {rnn_type}, hidden={n_hidden}, layers={n_layers}\")\\n    \\n    model = CharRNN(n_letters, n_hidden, len(dataset.labels_uniq), rnn_type=rnn_type, n_layers=n_layers, extra_layers=extra_layers)\\n    model.to(device)\\n    \\n    losses = train(model, train_s, n_epoch=n_epoch, n_batch_size=n_batch_size, learning_rate=learning_rate, report_every=max(1, n_epoch//5))\\n    \\n    # Accuracy on test set\\n    model.eval()\\n    correct = 0\\n    with torch.no_grad():\\n        for i in range(len(test_s)):\\n            (label_tensor, text_tensor, label, text) = test_s[i]\\n            output = model(text_tensor)\\n            guess, guess_i = label_from_output(output, dataset.labels_uniq)\\n            if guess == label: correct += 1\\n    \\n    print(f\"Validation Accuracy: {correct/len(test_s):.4f}\")\\n    return model, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k2lSwuS4lMd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t8WNvNN4psG"
      },
      "outputs": [],
      "source": [
        "def evaluate(rnn, testing_data, classes):\n",
        "    confusion = torch.zeros(len(classes), len(classes))\n",
        "\n",
        "    rnn.eval() #set to eval mode\n",
        "    with torch.no_grad(): # do not record the gradients during eval phase\n",
        "        for i in range(len(testing_data)):\n",
        "            (label_tensor, text_tensor, label, text) = testing_data[i]\n",
        "            output = rnn(text_tensor)\n",
        "            guess, guess_i = label_from_output(output, classes)\n",
        "            label_i = classes.index(label)\n",
        "            confusion[label_i][guess_i] += 1\n",
        "\n",
        "    # Normalize by dividing every row by its sum\n",
        "    for i in range(len(classes)):\n",
        "        denom = confusion[i].sum()\n",
        "        if denom > 0:\n",
        "            confusion[i] = confusion[i] / denom\n",
        "\n",
        "    # Set up plot\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(confusion.cpu().numpy()) #numpy uses cpu here so we need to use a cpu version\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
        "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
        "\n",
        "    # Force label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    # sphinx_gallery_thumbnail_number = 2\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "evaluate(rnn, test_set, classes=alldata.labels_uniq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment_with_data(alldata, rnn_type='GRU', n_layers=2, n_hidden = 256, n_epoch = 27)"
      ],
      "metadata": {
        "id": "bb8PqLeFB6md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment_with_data(alldata, rnn_type='GRU', n_layers=2, n_hidden = 128, n_epoch = 27)"
      ],
      "metadata": {
        "id": "6Adxy1DYBNR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment_with_data(alldata, rnn_type='LSTM', n_layers=2,n_hidden = 128, n_epoch = 27)"
      ],
      "metadata": {
        "id": "ecz21sOFBkfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_data = NamesDataset(file_path=\"reddit.tsv\", delimiter=\"\\t\", label_col=1, line_col=0)\n",
        "run_experiment_with_data(reddit_data, rnn_type='GRU', n_epoch=20, learning_rate=0.1)"
      ],
      "metadata": {
        "id": "d2AC6EeUCAso"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}